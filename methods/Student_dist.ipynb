{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Student_dist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAVwebWLAlb4RqqvD3XBnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kryuchkovdm/Distillation/blob/master/methods/Student_dist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbeEnllXC71X"
      },
      "source": [
        "def train_epoch(train_iter, model, optim, epoch_num, distil=False,alpha = 0.5,temperature=10):\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    if distil:\n",
        "        kldloss = nn.KLDivLoss()  \n",
        "        cost = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in tqdm(train_iter, total=len(train_iter), desc=f\"Batch progress for epoch {epoch_num}\"):\n",
        "        \n",
        "        batch = tuple([t.to(device) for t in batch])\n",
        "        inputs = {\"input_ids\": batch[0],\n",
        "                  \"attention_mask\": batch[1]}\n",
        "        labels = batch[3]\n",
        "\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        student_logits = model(inputs.get('input_ids'))\n",
        "        log = student_logits\n",
        "        soft_predictions = F.log_softmax( log / temperature, dim=1 )\n",
        "\n",
        "        if distil:\n",
        "            target = batch[4]\n",
        "            tar = target.clone()\n",
        "            teacher_logits = F.softmax( tar / temperature, dim=1 )\n",
        "            distillation_loss = kldloss(soft_predictions, teacher_logits)\n",
        "            target = labels\n",
        "        else:\n",
        "            target = labels\n",
        "\n",
        "        batch_loss = cost(student_logits, target)\n",
        "\n",
        "\n",
        "        if torch.isnan(batch_loss):\n",
        "            print(\"NAN batch loss!\", epoch_num, batch_loss, student_logits, target)\n",
        "        if distil:\n",
        "            train_loss += batch_loss.item()*(1-alpha) + distillation_loss.item()*(alpha)\n",
        "        else:\n",
        "            train_loss += batch_loss.item()\n",
        "\n",
        "        batch_acc = (student_logits.argmax(1) == labels).sum().item()\n",
        "        train_acc += batch_acc\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(student_logits.argmax(1).tolist())\n",
        "\n",
        "        batch_loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    return train_loss / len(train_iter), train_acc / len(train_iter.dataset), f1_score(y_true, y_pred, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzs1casuDCZp"
      },
      "source": [
        "def validate(test_iter, model):\n",
        "    test_acc = 0 \n",
        "    test_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    cost = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(test_iter, desc=\"Validating\"):\n",
        "        \n",
        "        batch = tuple([t.to(device) for t in batch])\n",
        "        inputs = {\"input_ids\": batch[0],\n",
        "                  \"attention_mask\": batch[1],\n",
        "                  \"token_type_ids\": batch[2]}\n",
        "        labels = batch[3]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            output = model(inputs.get('input_ids'))\n",
        "            batch_loss = cost(output, labels)\n",
        "            test_loss += batch_loss.item()\n",
        "                    \n",
        "            batch_acc = (output.argmax(1) == labels).sum().item() \n",
        "            test_acc += batch_acc\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(output.argmax(1).tolist())\n",
        "\n",
        "    return test_loss / len(test_iter), test_acc / len(test_iter.dataset), f1_score(y_true, y_pred, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEhA0loJDGkM"
      },
      "source": [
        "def train_loop(model, optim, train_loader, test_loader, n_epochs=5, sched=None, distil=False,alpha = 0.5,temperature=10):\n",
        "    training_results = {\"epoch\": list(range(n_epochs)),\n",
        "                        \"train_loss\": [],\n",
        "                        \"train_acc\": [],\n",
        "                        \"train_f1_macro\": [],\n",
        "                        \"test_loss\": [],\n",
        "                        \"test_acc\": [],\n",
        "                        \"test_f1_macro\": []}\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    try:\n",
        "        for i in range(n_epochs):\n",
        "            \n",
        "            train_loss, train_acc, train_f1 = train_epoch(train_loader, model, optim, epoch_num=i, distil=distil,alpha = alpha,temperature=temperature)\n",
        "            if sched is not None:\n",
        "                sched.step()\n",
        "            test_loss, test_acc, test_f1 = validate(test_loader, model)\n",
        "            training_results[\"train_loss\"].append(train_loss)\n",
        "            training_results[\"train_acc\"].append(train_acc)\n",
        "            training_results[\"train_f1_macro\"].append(train_f1)\n",
        "            training_results[\"test_loss\"].append(test_loss)\n",
        "            training_results[\"test_acc\"].append(test_acc)\n",
        "            training_results[\"test_f1_macro\"].append(test_f1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "    return pd.DataFrame(training_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}